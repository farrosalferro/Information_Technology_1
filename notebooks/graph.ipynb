{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac514875",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np       \n",
    "import pandas as pd      \n",
    "import os                \n",
    "from tqdm.notebook import tqdm  \n",
    "import gc                \n",
    "from copy import deepcopy \n",
    "import sys               \n",
    "\n",
    "# Custom modules from project structure\n",
    "# Import individual modules to prevent circular dependency issues\n",
    "from scripts.utils import dataset, camera, image, metric, submission\n",
    "from scripts.features import extraction, clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7018838c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constants defined. Using SIFT features and FLANN matcher.\n",
      "Data Directory: ../data/image-matching-challenge-2025\n"
     ]
    }
   ],
   "source": [
    "# Root data directory and output configuration\n",
    "DATA_DIR = \"../data/image-matching-challenge-2025\"\n",
    "OUTPUT_FILE = \"output_graph.csv\" # Final submission CSV with scene clusters and camera poses\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, \"train\")\n",
    "\n",
    "# --- Feature Extraction Parameters ---\n",
    "# Controls keypoint detection and descriptor computation\n",
    "FEATURE_EXTRACTOR_TYPE = 'SIFT'  # Scale-Invariant Feature Transform - robust to scale/rotation changes\n",
    "SIFT_NFEATURES = 8000  # Higher values capture more detail but increase memory and processing time\n",
    "\n",
    "# --- Matching Parameters ---\n",
    "# Controls how feature descriptors are matched between images\n",
    "MATCHER_TYPE = 'FLANN'  # Fast Library for Approximate Nearest Neighbors - efficient for large feature sets\n",
    "LOWE_RATIO_TEST_THRESHOLD = 0.8  # Lower values = stricter matching (fewer false positives, more false negatives)\n",
    "MIN_INLIER_MATCHES_INITIAL = 15  # Minimum matches required for initial geometric verification\n",
    "MIN_INLIER_MATCHES_GRAPH = 10  # Threshold for adding an edge to view graph (connectivity)\n",
    "\n",
    "# --- Geometric Verification Parameters ---\n",
    "# Controls outlier rejection using RANSAC for fundamental matrix estimation\n",
    "RANSAC_THRESHOLD = 1.5  # Maximum distance in pixels for a point to be considered an inlier\n",
    "\n",
    "# --- Clustering Parameters ---\n",
    "# Determines how images are grouped into scenes\n",
    "CLUSTERING_ALGORITHM = 'ConnectedComponents'  # Groups images based on view graph connectivity\n",
    "MIN_CLUSTER_SIZE = 3  # Minimum images required to consider a group as a valid scene\n",
    "\n",
    "# --- Structure from Motion (SfM) Parameters ---\n",
    "# Controls 3D reconstruction and camera pose estimation\n",
    "MIN_VIEWS_FOR_TRIANGULATION = 2  # Minimum camera views needed to triangulate a 3D point\n",
    "PNP_RANSAC_THRESHOLD = 5.0  # Reprojection error threshold (pixels) for PnP pose estimation\n",
    "PNP_CONFIDENCE = 0.999  # Confidence level for RANSAC in PnP (higher = more iterations)\n",
    "MIN_3D_POINTS_FOR_PNP = 6  # Minimum 3D-2D correspondences needed for reliable pose estimation\n",
    "\n",
    "# Camera model approximation (when calibration is unknown)\n",
    "DEFAULT_FOCAL_LENGTH_FACTOR = 1.2  # Focal length approximation as factor of image dimension\n",
    "\n",
    "print(f\"Constants defined. Using {FEATURE_EXTRACTOR_TYPE} features and {MATCHER_TYPE} matcher.\")\n",
    "print(f\"Data Directory: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66f49fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset \"imc2023_haiper\" -> num_images=54\n",
      "Dataset \"imc2023_heritage\" -> num_images=209\n",
      "Dataset \"imc2023_theather_imc2024_church\" -> num_images=76\n",
      "Dataset \"imc2024_dioscuri_baalshamin\" -> num_images=138\n",
      "Dataset \"imc2024_lizard_pond\" -> num_images=214\n",
      "Dataset \"pt_brandenburg_british_buckingham\" -> num_images=225\n",
      "Dataset \"pt_piazzasanmarco_grandplace\" -> num_images=168\n",
      "Dataset \"pt_sacrecoeur_trevi_tajmahal\" -> num_images=225\n",
      "Dataset \"pt_stpeters_stpauls\" -> num_images=200\n",
      "Dataset \"amy_gardens\" -> num_images=200\n",
      "Dataset \"fbk_vineyard\" -> num_images=163\n",
      "Dataset \"ETs\" -> num_images=22\n",
      "Dataset \"stairs\" -> num_images=51\n"
     ]
    }
   ],
   "source": [
    "# Load dataset samples from label file into structured format\n",
    "# Returns a dictionary mapping dataset names to lists of image metadata\n",
    "samples = dataset.load_dataset(DATA_DIR)\n",
    "\n",
    "# Display summary of loaded datasets and their image counts\n",
    "for dataset_name in samples:\n",
    "    print(f'Dataset \"{dataset_name}\" -> num_images={len(samples[dataset_name])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a411ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import matching module separately to avoid circular import issues in dependency graph\n",
    "from scripts.features import matching\n",
    "\n",
    "def process_dataset(dataset_id, test_image_dir, predictions, extractor, matcher):\n",
    "    \"\"\"\n",
    "    Process a complete image dataset through the full SfM pipeline:\n",
    "    1. Extract features from all images\n",
    "    2. Build view graph based on feature matching\n",
    "    3. Cluster images into scenes using connected components\n",
    "    4. Handle outlier images (those not belonging to any cluster)\n",
    "    5. Run Structure from Motion per cluster to compute camera poses\n",
    "    \n",
    "    Args:\n",
    "        dataset_id: Name/ID of the dataset being processed\n",
    "        test_image_dir: Root directory containing dataset folders\n",
    "        predictions: List of prediction objects to be updated with results\n",
    "        extractor: Initialized feature extractor object\n",
    "        matcher: Initialized feature matcher object\n",
    "        \n",
    "    Returns:\n",
    "        Updated predictions list with cluster assignments and camera poses\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Processing Dataset: {dataset_id} ---\")\n",
    "\n",
    "    dataset_path = os.path.join(test_image_dir, dataset_id)\n",
    "    filename_to_index = {p.filename: idx for idx, p in enumerate(predictions)}\n",
    "    \n",
    "    # STEP 1: Extract Features from each image in the dataset\n",
    "    # Returns dict mapping image_id -> features and image dimensions\n",
    "    extracted_features, image_dims = extraction.load_and_extract_features_dataset(dataset_id, test_image_dir, extractor)\n",
    "    image_ids_in_dataset = list(extracted_features.keys())\n",
    "\n",
    "    # STEP 2: Build View Graph representing image connectivity\n",
    "    # Nodes = images, Edges = sufficient feature matches between image pairs\n",
    "    # Also returns pairwise_matches dict with geometric verification results\n",
    "    G, pairwise_matches = clustering.build_view_graph(image_ids_in_dataset, extracted_features, matcher)\n",
    "\n",
    "    # STEP 3: Cluster Images into scenes based on view graph connectivity\n",
    "    # Returns list of clusters (each a list of image IDs) and list of outlier image IDs\n",
    "    clusters, outliers = clustering.cluster_images(G, algorithm=CLUSTERING_ALGORITHM, min_cluster_size=MIN_CLUSTER_SIZE)\n",
    "\n",
    "    # STEP 4: Process Outliers - images not belonging to any valid cluster\n",
    "    # Set null pose matrices and assign to 'outliers' cluster\n",
    "    print(f\"Marking {len(outliers)} images as outliers.\")\n",
    "    for img_id in outliers:\n",
    "        r, t = camera.format_pose(None, None)\n",
    "        prediction_index = filename_to_index[img_id]\n",
    "        predictions[prediction_index].cluster_index = \"outliers\"\n",
    "        predictions[prediction_index].rotation = r\n",
    "        predictions[prediction_index].translation = t\n",
    "\n",
    "    # STEP 5: Run Structure from Motion (SfM) for each identified cluster\n",
    "    print(f\"Running SfM for {len(clusters)} clusters...\")\n",
    "    for i, cluster_nodes in enumerate(clusters):\n",
    "        cluster_label = f\"cluster{i+1}\"\n",
    "        print(f\"\\nProcessing {cluster_label} ({len(cluster_nodes)} images)...\")\n",
    "\n",
    "        # Filter data to only include images in the current cluster (memory optimization)\n",
    "        cluster_features = {img_id: extracted_features[img_id] for img_id in cluster_nodes if img_id in extracted_features}\n",
    "        cluster_dims = {img_id: image_dims[img_id] for img_id in cluster_nodes if img_id in image_dims}\n",
    "        \n",
    "        # Filter pairwise matches to only include image pairs within this cluster\n",
    "        cluster_pairwise_matches = {}\n",
    "        for (id1, id2), matches in pairwise_matches.items():\n",
    "             if id1 in cluster_nodes and id2 in cluster_nodes:\n",
    "                 cluster_pairwise_matches[(id1, id2)] = matches\n",
    "\n",
    "        # Estimate camera poses through triangulation and PnP for this cluster\n",
    "        cluster_poses = camera.estimate_poses_for_cluster(\n",
    "            cluster_nodes,\n",
    "            cluster_features,\n",
    "            cluster_dims,\n",
    "            matcher,\n",
    "            cluster_pairwise_matches # Pass filtered matches\n",
    "        )\n",
    "\n",
    "        # Update predictions with cluster assignments and camera poses\n",
    "        for img_id in cluster_nodes:\n",
    "            R, T = cluster_poses.get(img_id, (None, None)) # Get pose, default to None if not found\n",
    "            r, t = camera.format_pose(R, T)\n",
    "            prediction_index = filename_to_index[img_id]\n",
    "            predictions[prediction_index].cluster_index = cluster_label\n",
    "            predictions[prediction_index].rotation = deepcopy(r)\n",
    "            predictions[prediction_index].translation = deepcopy(t)\n",
    "\n",
    "        # Clean up memory to prevent OOM errors when processing large datasets\n",
    "        del cluster_features, cluster_dims, cluster_poses, cluster_pairwise_matches\n",
    "        gc.collect()\n",
    "\n",
    "    print(f\"--- Finished Processing Dataset: {dataset_id} ---\")\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59070e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Processing Train Datasets ===\n",
      "\n",
      "--- Processing Dataset: ETs ---\n",
      "Extracting features for 22 images in dataset ETs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Features ETs: 100%|██████████| 22/22 [00:00<00:00, 43.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building view graph for 22 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matching pairs: 100%|██████████| 231/231 [00:04<00:00, 52.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View graph built with 22 nodes and 64 edges.\n",
      "Clustering graph using ConnectedComponents...\n",
      "Found 1 clusters and 3 potential outliers.\n",
      "Marking 3 images as outliers.\n",
      "Running SfM for 1 clusters...\n",
      "\n",
      "Processing cluster1 (19 images)...\n",
      "Initializing SfM with pair (another_et_another_et002.png, another_et_another_et001.png) with 484 matches.\n",
      "Essential matrix inliers: 436\n",
      "Triangulated 423 initial 3D points.\n",
      "Attempting to register 17 remaining images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registering images: 100%|██████████| 17/17 [00:00<00:00, 495.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully registered image another_et_another_et005.png (113 PnP inliers).\n",
      "Successfully registered image another_et_another_et004.png (109 PnP inliers).\n",
      "Successfully registered image another_et_another_et003.png (57 PnP inliers).\n",
      "Successfully registered image another_et_another_et006.png (38 PnP inliers).\n",
      "Successfully registered image another_et_another_et007.png (20 PnP inliers).\n",
      "Finished SfM for cluster. Registered 7 out of 19 images.\n",
      "--- Finished Processing Dataset: ETs ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Main Pipeline Execution ---\n",
    "# Process train datasets to verify algorithm performance against ground truth\n",
    "if os.path.isdir(TRAIN_DIR):\n",
    "    # List all dataset directories and sort by image count (process smaller datasets first)\n",
    "    # This helps catch issues early before committing to larger datasets\n",
    "    train_datasets = [os.path.basename(os.path.join(TRAIN_DIR, d)) for d in os.listdir(TRAIN_DIR) \n",
    "                     if os.path.isdir(os.path.join(TRAIN_DIR, d))]\n",
    "    train_datasets.sort(key=lambda x: len(os.listdir(os.path.join(TRAIN_DIR, x))), reverse=False)\n",
    "    \n",
    "    print(\"=== Processing Train Datasets ===\")\n",
    "    # Initialize core components for the pipeline\n",
    "    extractor = extraction.get_feature_extractor('SIFT', SIFT_NFEATURES)\n",
    "    matcher = matching.get_matcher('FLANN', 'SIFT')\n",
    "    \n",
    "    # Process datasets (limiting to first dataset for demonstration/development)\n",
    "    # Remove the slice [:1] to process all datasets\n",
    "    for dataset_name in train_datasets[:1]:\n",
    "        samples[dataset_name] = process_dataset(dataset_name, TRAIN_DIR, samples[dataset_name], extractor, matcher)\n",
    "else:\n",
    "    print(\"Train directory not found - check data path configuration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fa9016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate submission file in required format for evaluation\n",
    "# Format: dataset,image,cluster_index,rotation_matrix,translation_vector\n",
    "submission.create_submission_file(samples, OUTPUT_FILE)\n",
    "\n",
    "# Display first few rows of submission file to verify format correctness\n",
    "!head {OUTPUT_FILE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6e2753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate results against ground truth using competition metrics\n",
    "# Two key metrics are measured:\n",
    "# 1. Clustering accuracy (scene assignment correctness)\n",
    "# 2. Camera pose estimation accuracy (rotation and translation error)\n",
    "\n",
    "final_score, dataset_scores = metric.score(\n",
    "    gt_csv=os.path.join(DATA_DIR, \"train_labels.csv\"),  # Ground truth from training set\n",
    "    user_csv=OUTPUT_FILE,                               # Our predictions\n",
    "    thresholds_csv=os.path.join(DATA_DIR, \"train_thresholds.csv\"),  # Dataset-specific thresholds\n",
    "    mask_csv=None,                                      # Optional subset evaluation\n",
    "    inl_cf=0,                                           # Inlier confidence factor\n",
    "    strict_cf=-1,                                       # Strictness confidence factor\n",
    "    verbose=True,                                       # Show detailed results\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
