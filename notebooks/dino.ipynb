{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5690851",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "# DINO Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40763149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "from time import sleep, time\n",
    "\n",
    "import kornia as K\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pycolmap\n",
    "from matplotlib.lines import Line2D\n",
    "from sklearn.metrics import silhouette_score\n",
    "from IPython.display import clear_output\n",
    "from scripts import utils, database, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb94f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = K.utils.get_cuda_device_if_available(0)\n",
    "print(f\"{device=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79956a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data/image-matching-challenge-2025\"\n",
    "DINO_DIR = \"weights/dinov2\"\n",
    "OUTPUT_FILE = \"train_predictions.csv\"\n",
    "\n",
    "DB_IMG_EXT = \"\"\n",
    "DB_CAMERA_MODEL = \"simple-pinhole\"\n",
    "\n",
    "# Configure dataset filtering \n",
    "DATASETS_FILTER = [\n",
    "    # New 2025 datasets\n",
    "    \"amy_gardens\",\n",
    "    \"ETs\",\n",
    "    \"fbk_vineyard\",\n",
    "    \"stairs\",\n",
    "    # Data from IMC 2023 and 2024.\n",
    "    'imc2024_dioscuri_baalshamin',\n",
    "    'imc2023_theather_imc2024_church',\n",
    "    'imc2023_heritage',\n",
    "    'imc2023_haiper',\n",
    "    'imc2024_lizard_pond',\n",
    "    # Crowdsourced PhotoTourism data.\n",
    "    'pt_stpeters_stpauls',\n",
    "    'pt_brandenburg_british_buckingham',\n",
    "    'pt_piazzasanmarco_grandplace',\n",
    "    'pt_sacrecoeur_trevi_tajmahal',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9960e2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_DIR, \"train_labels.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23198011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "samples = utils.dataset.load_dataset(DATA_DIR)\n",
    "\n",
    "for dataset in samples:\n",
    "    print(f'Dataset \"{dataset}\" -> num_images={len(samples[dataset])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b57649",
   "metadata": {},
   "source": [
    "## Clustering Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1020073b",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {}\n",
    "for dataset in df['dataset'].unique():\n",
    "    datasets[dataset] = {}\n",
    "    dataset_rows = df[df['dataset'] == dataset]\n",
    "    for scene in dataset_rows['scene'].unique():\n",
    "        datasets[dataset][scene] = []\n",
    "        scene_rows = dataset_rows[dataset_rows['scene'] == scene]\n",
    "        image_paths = [os.path.join(DATA_DIR, 'train', dataset, path) for path in scene_rows['image']]\n",
    "        print(f\"Generating descriptors for {dataset}/{scene} -> num_images={len(image_paths)}\")\n",
    "        global_descriptors = features.extraction.extract_cls_descriptor_dino(\n",
    "            image_paths,\n",
    "            dino_path=DINO_DIR,\n",
    "            device=device,\n",
    "            normalize=True\n",
    "        )\n",
    "        datasets[dataset][scene] = global_descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2734ddf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vectors = {dataset_name: [] for dataset_name in datasets.keys()}\n",
    "all_labels = {dataset_name: [] for dataset_name in datasets.keys()}\n",
    "all_label_names = {dataset_name: [] for dataset_name in datasets.keys()}\n",
    "\n",
    "for dataset_name, scenes in datasets.items():\n",
    "    for cluster_id, (name, vectors) in enumerate(scenes.items()):\n",
    "        all_vectors[dataset_name].extend([v.cpu().numpy() for v in vectors])\n",
    "        all_labels[dataset_name].extend([cluster_id] * len(vectors))\n",
    "        all_label_names[dataset_name].extend([name] * len(vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13c2d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name, image_features in all_vectors.items():\n",
    "    # Step 1: t-SNE on features\n",
    "    features_2d = features.extraction.feature_reducer(\n",
    "            algorithm=\"TSNE\",\n",
    "            features=np.vstack(image_features),\n",
    "            n_components=2,\n",
    "            random_state=42,\n",
    "            perplexity=15\n",
    "        )\n",
    "\n",
    "    # Step 2: Run HDBSCAN or any clustering on 2D features\n",
    "    cluster_labels = features.clustering.dino_clusterer(\n",
    "            algorithm=\"HDBSCAN\",\n",
    "            features=features_2d,\n",
    "            scaler=None,\n",
    "            min_cluster_size=2,\n",
    "        )\n",
    "    silhouette_avg = silhouette_score(features_2d, cluster_labels)\n",
    "    print(f\"Dataset: {dataset_name}, Silhouette Score: {silhouette_avg:.4f}\")\n",
    "\n",
    "    # Step 3: Prepare GT label color map\n",
    "    gt_labels = all_label_names[dataset_name]  # Make sure this is aligned with image_features\n",
    "    cluster_names = sorted(set(gt_labels))\n",
    "    gt_palette = sns.color_palette(\"hls\", len(cluster_names))\n",
    "    gt_color_map = {name: gt_palette[i] for i, name in enumerate(cluster_names)}\n",
    "    gt_colors = [gt_color_map[label] for label in gt_labels]\n",
    "\n",
    "    # Step 4: Prepare prediction label color map\n",
    "    unique_labels = sorted(set(cluster_labels))\n",
    "    pred_palette = sns.color_palette(\"hls\", len(unique_labels))\n",
    "    pred_color_map = {label: pred_palette[i] for i, label in enumerate(unique_labels)}\n",
    "    pred_colors = [pred_color_map[label] for label in cluster_labels]\n",
    "\n",
    "    # ---- Plotting both subplots ----\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(18, 8))\n",
    "    fig.suptitle(f\"t-SNE Clustering: {dataset_name}\", fontsize=16)\n",
    "\n",
    "    # --- Left: Prediction ---\n",
    "    ax = axs[0]\n",
    "    for label in unique_labels:\n",
    "        indices = [i for i, l in enumerate(cluster_labels) if l == label]\n",
    "        subset = features_2d[indices]\n",
    "        ax.scatter(subset[:, 0], subset[:, 1],\n",
    "                   c=[pred_color_map[label]],\n",
    "                   label=f'Cluster {label}' if label != -1 else 'Outlier',\n",
    "                   s=40, alpha=0.6, edgecolors='k')\n",
    "    ax.set_title(\"Predicted Clusters\")\n",
    "    ax.set_xlabel(\"t-SNE 1\")\n",
    "    ax.set_ylabel(\"t-SNE 2\")\n",
    "    ax.legend(fontsize='small')\n",
    "\n",
    "    # --- Right: Ground Truth ---\n",
    "    ax = axs[1]\n",
    "    for name in cluster_names:\n",
    "        indices = [i for i, lbl in enumerate(gt_labels) if lbl == name]\n",
    "        subset = features_2d[indices]\n",
    "        ax.scatter(subset[:, 0], subset[:, 1],\n",
    "                   c=[gt_color_map[name]],\n",
    "                   label=name,\n",
    "                   s=40, alpha=0.6, edgecolors='k')\n",
    "    ax.set_title(\"Ground Truth Clusters\")\n",
    "    ax.set_xlabel(\"t-SNE 1\")\n",
    "    ax.set_ylabel(\"t-SNE 2\")\n",
    "\n",
    "    # GT Legend\n",
    "    handles = [\n",
    "        Line2D([0], [0], marker='o', color='w', label=label,\n",
    "               markerfacecolor=gt_color_map[label], markersize=8, markeredgecolor='k')\n",
    "        for label in cluster_names\n",
    "    ]\n",
    "    ax.legend(handles=handles, bbox_to_anchor=(1.05, 1), loc='upper left', fontsize='small', title='GT')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de274969",
   "metadata": {},
   "source": [
    "## Do Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b849ebe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear memory to prevent OOM errors\n",
    "gc.collect()\n",
    "mapping_result_strs = []  # Store results for each dataset\n",
    "\n",
    "print(f\"Extracting on device {device}\")\n",
    "# Process each dataset\n",
    "for dataset, predictions in samples.items():\n",
    "    # Skip datasets not in filter list\n",
    "    if DATASETS_FILTER and dataset not in DATASETS_FILTER:\n",
    "        print(f'Skipping \"{dataset}\"')\n",
    "        continue\n",
    "\n",
    "    # Setup paths and image lists\n",
    "    images_dir = os.path.join(DATA_DIR, \"train\", dataset)\n",
    "    images = [os.path.join(images_dir, p.filename) for p in predictions]\n",
    "\n",
    "    print(f'\\nProcessing dataset \"{dataset}\": {len(images)} images')\n",
    "\n",
    "    # Map filenames to prediction indices\n",
    "    filename_to_index = {p.filename: idx for idx, p in enumerate(predictions)}\n",
    "\n",
    "    # Setup output directory for features\n",
    "    feature_dir = os.path.join(\"featureout\", dataset)\n",
    "    os.makedirs(feature_dir, exist_ok=True)\n",
    "\n",
    "    # Wrap algos in try-except blocks so we can populate a submission even if one scene crashes.\n",
    "    try:\n",
    "        # 1. Image pair selection using DINO features\n",
    "        t = time()\n",
    "        cls_descriptor_dino = features.extraction.extract_cls_descriptor_dino(\n",
    "            images,\n",
    "            dino_path=DINO_DIR,\n",
    "            device=device,\n",
    "            normalize=True\n",
    "        )\n",
    "        reduced_features = features.extraction.feature_reducer(\n",
    "            algorithm=\"PCA\",\n",
    "            features=cls_descriptor_dino.cpu().numpy(),\n",
    "            n_components=50,\n",
    "            scaler=None,\n",
    "            random_state=42\n",
    "        )\n",
    "        cluster_labels = features.clustering.dino_clusterer(\n",
    "            algorithm=\"HDBSCAN\",\n",
    "            features=reduced_features,\n",
    "            scaler=None,\n",
    "            min_cluster_size=10,\n",
    "            metric='euclidean',\n",
    "            cluster_selection_method='eom',\n",
    "        )\n",
    "        print(\n",
    "            f\"Clustering. Number of clusters: {np.unique(cluster_labels)}, with {sum(cluster_labels == -1)} outliers. Done in {time() - t:.4f} sec\"\n",
    "        )\n",
    "        gc.collect()\n",
    "        images_np = np.array(images)[cluster_labels != -1]\n",
    "        cluster_labels = cluster_labels[cluster_labels != -1]\n",
    "        for cluster in np.unique(cluster_labels):\n",
    "            cluster_images = images_np[cluster_labels == cluster]\n",
    "            feature_dir_cluster = os.path.join(feature_dir, f\"cluster_{cluster}\")\n",
    "            os.makedirs(feature_dir_cluster, exist_ok=True)\n",
    "\n",
    "            print(f\"Processing Cluster {cluster}: {len(cluster_images)} images\")\n",
    "\n",
    "            index_pairs = features.matching.get_image_pairs_shortlist_dino(\n",
    "                cluster_images.tolist(),\n",
    "                dino_path=DINO_DIR,\n",
    "                sim_th=0.3,  # Strict similarity threshold\n",
    "                min_pairs=20,  # Minimum pairs per image with biggest similarity\n",
    "                exhaustive_if_less=20,\n",
    "                device=device,\n",
    "            )\n",
    "            print(\n",
    "                f\"Shortlisting. Number of pairs to match: {len(index_pairs)}. Done in {time() - t:.4f} sec\"\n",
    "            )\n",
    "            gc.collect()\n",
    "\n",
    "            # 2. Local feature detection with ALIKED\n",
    "            t = time()\n",
    "            features.extraction.detect_keypoint_aliked(images, feature_dir_cluster, 4096, device=device)\n",
    "            gc.collect()\n",
    "            print(f\"Features detected in {time() - t:.4f} sec\")\n",
    "\n",
    "            # 3. Feature matching with LightGlue\n",
    "            t = time()\n",
    "            features.matching.match_keypoint_lightglue(\n",
    "                images, index_pairs, feature_dir=feature_dir_cluster, device=device, verbose=False\n",
    "            )\n",
    "            print(f\"Features matched in {time() - t:.4f} sec\")\n",
    "\n",
    "            # 4. Create/reset COLMAP database\n",
    "            database_path = os.path.join(feature_dir_cluster, \"colmap.db\")\n",
    "            if os.path.isfile(database_path):\n",
    "                os.remove(database_path)\n",
    "            gc.collect()\n",
    "            sleep(1)\n",
    "            # Import features and matches into COLMAP format\n",
    "            database.h5_to_db.import_into_colmap(\n",
    "                images_dir,\n",
    "                DB_CAMERA_MODEL,\n",
    "                img_ext=DB_IMG_EXT,\n",
    "                feature_dir=feature_dir_cluster,\n",
    "                database_path=database_path,\n",
    "            )\n",
    "            output_path = f\"{feature_dir_cluster}/colmap_rec_aliked\"\n",
    "\n",
    "            # 5. Geometric verification with RANSAC\n",
    "            t = time()\n",
    "            pycolmap.match_exhaustive(database_path)\n",
    "            print(f\"Ran RANSAC in {time() - t:.4f} sec\")\n",
    "\n",
    "            # 6. SfM reconstruction with COLMAP\n",
    "            # Configure reconstruction parameters\n",
    "            mapper_options = pycolmap.IncrementalPipelineOptions()\n",
    "            mapper_options.min_model_size = 3  # Allow small reconstructions (min 3 images). Colmap by default does not generate a reconstruction if <10 images are registered.\n",
    "            mapper_options.max_num_models = 25  # Limit number of separate models\n",
    "            os.makedirs(output_path, exist_ok=True)\n",
    "            t = time()\n",
    "            maps = pycolmap.incremental_mapping(\n",
    "                database_path=database_path,\n",
    "                image_path=images_dir,\n",
    "                output_path=output_path,\n",
    "                options=mapper_options,\n",
    "            )\n",
    "            sleep(1)\n",
    "            print(f\"Reconstruction done in  {time() - t:.4f} sec\")\n",
    "            print(maps)\n",
    "\n",
    "            clear_output(wait=False)\n",
    "\n",
    "            # 7. Extract poses from reconstruction\n",
    "            registered = 0\n",
    "            for map_index, cur_map in maps.items():\n",
    "                for _, image in cur_map.images.items():\n",
    "                    prediction_index = filename_to_index[image.name]\n",
    "                    # predictions[prediction_index].cluster_index = map_index\n",
    "                    predictions[prediction_index].cluster_index = cluster\n",
    "                    predictions[prediction_index].rotation = deepcopy(\n",
    "                        image.cam_from_world.rotation.matrix()\n",
    "                    )\n",
    "                    predictions[prediction_index].translation = deepcopy(\n",
    "                        image.cam_from_world.translation\n",
    "                    )\n",
    "                    registered += 1\n",
    "            mapping_result_str = f'Dataset \"{dataset}\" -> Registered {registered} / {len(images)} images with {len(np.unique(cluster_labels))} clusters'\n",
    "            mapping_result_strs.append(mapping_result_str)\n",
    "            print(mapping_result_str)\n",
    "            print(f\"# clusters predicted by colmap: {len(maps)}\")\n",
    "            gc.collect()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        # raise e\n",
    "        mapping_result_str = f'Dataset \"{dataset}\" -> Failed!'\n",
    "        mapping_result_strs.append(mapping_result_str)\n",
    "        print(mapping_result_str)\n",
    "\n",
    "# Print summary of results\n",
    "print(\"\\nResults\")\n",
    "for s in mapping_result_strs:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3f1bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a submission file.\n",
    "utils.submission.create_submission_file(samples, OUTPUT_FILE)\n",
    "\n",
    "!head {OUTPUT_FILE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b627dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute results if running on the training set.\n",
    "# Don't do this when submitting a notebook for scoring. All you have to do is save your submission to /kaggle/working/submission.csv.\n",
    "\n",
    "t = time()\n",
    "final_score, dataset_scores = utils.metric.score(\n",
    "    gt_csv=os.path.join(DATA_DIR, \"train_labels.csv\"),\n",
    "    user_csv=OUTPUT_FILE,\n",
    "    thresholds_csv=os.path.join(DATA_DIR, \"train_thresholds.csv\"),\n",
    "    mask_csv=None,\n",
    "    inl_cf=0,\n",
    "    strict_cf=-1,\n",
    "    verbose=True,\n",
    ")\n",
    "print(f\"Computed metric in: {time() - t:.02f} sec.\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "it1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
